Step 1 Create a namespace.
To facilitate subsequent observation, run the following command to create a dedicated
namespace:
kubectl create ns csi
Step 2 Create a ConfigMap.
Run the following command on the Ceph node to query the information (in bold)
required by the ConfigMap:
[ceph: root@ceph01 /]# ceph mon dump
epoch 3
fsid 88ab034c-a622-11ee-9a8c-000c2905539b
last_changed 2023-12-29T08:27:15.558834+0000
created 2023-12-29T08:16:48.302982+0000
min_mon_release 16 (pacific)
election_strategy: 1
0: [v2:192.168.1.11:3300/0,v1:192.168.1.11:6789/0] mon.ceph01
1: [v2:192.168.1.12:3300/0,v1:192.168.1.12:6789/0] mon.ceph02
2: [v2:192.168.1.13:3300/0,v1:192.168.1.13:6789/0] mon.ceph03
In the ceph-rbd directory, modify the csi-config-map.yaml file based on the following
content (set ceph-csi-config based on the query result and do not directly copy the
example value):
---
apiVersion: v1
kind: ConfigMap
metadata:
 name: "ceph-csi-config"
data:
 config.json: |-
 [
 {
 "clusterID": "88ab034c-a622-11ee-9a8c-000c2905539b",
 "monitors": [
 "192.168.1.11:6789",
 "192.168.1.12:6789",
 "192.168.1.13:6789"
 ]
 }
 ]
---
apiVersion: v1
kind: ConfigMap
data:
 config.json: |-
 {}
metadata:
 name: ceph-csi-encryption-kms-config
---
apiVersion: v1
kind: ConfigMap
data:
 ceph.conf: |
 [global]
 auth_cluster_required = cephx
 auth_service_required = cephx
 auth_client_required = cephx
 keyring: |
metadata:
 name: ceph-config

After the modification, run the following command to deploy the ConfigMap:
kubectl apply -f csi-config-map.yaml -n csi
After the deployment is complete, you can query the following information:
Step 3 Create a secret.
On the Ceph node, query the information about the user of the connected RBD resource
pool. The user is created in step 1 in section 3.2. The detailed information is as follows:
[ceph: root@ceph01 /]# ceph auth get client.kubernetes
[client.kubernetes]
 key = AQAtBbNl/mm7AhAAXeVQDRVQ+oK2JIv95ZbHHQ==
 caps mgr = "profile rbd pool=kubernetes"
 caps mon = "profile rbd"
 caps osd = "profile rbd pool=kubernetes"
exported keyring for client.kubernetes
userID and userKey need to be specified in the secret.yaml file. The following is an
example secret.yaml file:
---
apiVersion: v1
kind: Secret
metadata:
 name: csi-rbd-secret
 namespace: csi
stringData:
 # Key values correspond to a user name and its key, as defined in the
 # ceph cluster. User ID should have required access to the 'pool'
 # specified in the storage class
 userID: kubernetes
 userKey: AQAtBbNl/mm7AhAAXeVQDRVQ+oK2JIv95ZbHHQ==
 # Encryption passphrase
 encryptionPassphrase: test_passphrase
Add a namespace. Then, use this file to deploy the secret. After the deployment is
complete, verify that the secret information can be queried. The details are as follows:


Step 4 Deploy ceph-csi.
Modify the namespace information in csi-provisioner-rbac.yaml, csi-nodepluginrbac.yaml, csi-rbdplugin-provisioner.yaml, and csi-rbdplugin.yaml. Change default to
csi. You are advised to use sed to modify the namespace information in batches because
there are multiple settings to be modified. After the modification, run the grep command
and check whether the files contain namespace: default. If the modifications are correct
and complete, run the following command to deploy ceph-csi:
kubectl apply -f csi-nodeplugin-rbac.yaml -f csi-provisioner-rbac.yaml -f csi-rbdpluginprovisioner.yaml -f csi-rbdplugin.yaml
After the operation is complete, you can query the Deployment related to ceph-csi. The
details are as follows:
kubectl get deplot -n csi

You can also query the following DaemonSet:
kubectl get daemonset -n csi

Verify that the information is correct before proceeding to the next step.


Step 5 Mount storage.
Mount a raw device (block) or a file system to a pod. In this lab, a raw device is used as
an example.
Create a StorageClass file csi-rbd-sc.yaml as follows:
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
 name: csi-rbd-sc
 namespace: csi
provisioner: rbd.csi.ceph.com
parameters:
 clusterID: 88ab034c-a622-11ee-9a8c-000c2905539b
 pool: kubernetes
 imageFeatures: layering
 csi.storage.k8s.io/provisioner-secret-name: csi-rbd-secret
 csi.storage.k8s.io/provisioner-secret-namespace: csi
 csi.storage.k8s.io/controller-expand-secret-name: csi-rbd-secret
 csi.storage.k8s.io/controller-expand-secret-namespace: csi
 csi.storage.k8s.io/node-stage-secret-name: csi-rbd-secret
 csi.storage.k8s.io/node-stage-secret-namespace: csi
reclaimPolicy: Delete
allowVolumeExpansion: true
mountOptions:
 - discard
Note: The namespace in this file must be the same as that of other object resources.
secret must be the same as that created in step 3 of this lab.
Run the kubectl apply -f csi-rbd-sc.yaml command for deployment.
Create a PVC file raw-block-pvc.yaml as follows:
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
 name: raw-block-pvc
 namespace: csi
spec:
 accessModes:
 - ReadWriteOnce
 volumeMode: Block
resources:
 requests:
 storage: 1Gi
 storageClassName: csi-rbd-sc


Note: The value of storageClassName in this file must be the same as that in csi-rbdsc.yaml.
Run the kubectl apply -f raw-block-pvc.yaml command for deployment.
Create a pod file raw-block-pod.yaml as follows:
---
apiVersion: v1
kind: Pod
metadata:
 name: pod-with-raw-block-volume
 namespace: csi
spec:
 containers:
 - name: fc-container
 image: nginx
 volumeDevices:
 - name: data
 devicePath: /dev/xvda
 volumes:
 - name: data
 persistentVolumeClaim:
 claimName: raw-block-pvc
Run the kubectl apply -f raw-block-pod.yaml command for deployment.

Step 6 Verify the configuration results.
On the Kubernetes node, verify that the PVC and PV are bound. The details are as
follows:
kubectl get pvc -n csi

On the Ceph node, verify that an image is created in the resource pool. The details are as
follows:
rbd -p kubernetes ls

According to the results, the RBD resource is successfully connected to Kubernetes.

⚫ Question: After the created pod and PVC are deleted, will the PV and the image in
the corresponding RBD pool be automatically deleted?
⚫ Answer: After the pod and PVC are deleted from Kubernetes, the image is
automatically deleted. The details are as follows:
[root@master01 ceph-csi]# kubectl delete -f raw-block-pvc.yaml
persistentvolumeclaim "raw-block-pvc" deleted
[root@master01 ceph-csi]# kubectl get pvc -n csi
No resources found in csi namespace.
[root@master01 ceph-csi]# kubectl get pv
No resources found
On the Ceph node, the corresponding image is automatically deleted.





